From 9748621ff64d13869057995f525083c69d517073 Mon Sep 17 00:00:00 2001
From: Marian-Cristian Rotariu <marian.rotariu@freescale.com>
Date: Wed, 28 Oct 2015 13:20:37 +0200
Subject: [PATCH 109/146] dpaa_eth: fix S/G for TX on ARM platforms

This patch adds a workaround for the 4K memory boundary limitation on ARM. If a
S/G entry exceeds the 4K boundary it is devided into two S/G entries.

Signed-off-by: Marian-Cristian Rotariu <marian.rotariu@freescale.com>
---
 .../net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c  | 114 ++++++++++++++++++++-
 1 file changed, 113 insertions(+), 1 deletion(-)

diff --git a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
index 4c92424..e35130b 100644
--- a/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
+++ b/drivers/net/ethernet/freescale/sdk_dpaa/dpaa_eth_sg.c
@@ -232,9 +232,18 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 
 	if (unlikely(fd->format == qm_fd_sg)) {
 		nr_frags = skb_shinfo(skb)->nr_frags;
+#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+/* addressing the 4k DMA issue can yield a larger number of fragments than
+ * the skb had
+ */
+		dma_unmap_single(dpa_bp->dev, addr, dpa_fd_offset(fd) +
+				 sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES,
+				 dma_dir);
+#else
 		dma_unmap_single(dpa_bp->dev, addr, dpa_fd_offset(fd) +
 				 sizeof(struct qm_sg_entry) * (1 + nr_frags),
 				 dma_dir);
+#endif
 		/* The sgt buffer has been allocated with netdev_alloc_frag(),
 		 * it's from lowmem.
 		 */
@@ -258,7 +267,15 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 		sg_addr = qm_sg_addr(&sgt[0]);
 		sg_len = qm_sg_entry_get_len(&sgt[0]);
 		dma_unmap_single(dpa_bp->dev, sg_addr, sg_len, dma_dir);
-
+#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+		i = 1;
+		do {
+			DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
+			sg_addr = qm_sg_addr(&sgt[i]);
+			sg_len = qm_sg_entry_get_len(&sgt[i]);
+			dma_unmap_page(dpa_bp->dev, sg_addr, sg_len, dma_dir);
+		} while (!qm_sg_entry_get_final(&sgt[i++]));
+#else
 		/* remaining pages were mapped with dma_map_page() */
 		for (i = 1; i <= nr_frags; i++) {
 			DPA_BUG_ON(qm_sg_entry_get_ext(&sgt[i]));
@@ -266,6 +283,7 @@ struct sk_buff *_dpa_cleanup_tx_fd(const struct dpa_priv_s *priv,
 			sg_len = qm_sg_entry_get_len(&sgt[i]);
 			dma_unmap_page(dpa_bp->dev, sg_addr, sg_len, dma_dir);
 		}
+#endif
 
 		/* Free the page frag that we allocated on Tx */
 		put_page(virt_to_head_page(sgt));
@@ -735,6 +753,10 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	struct net_device *net_dev = priv->net_dev;
 	int sg_len;
 	int err;
+#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+	dma_addr_t boundary;
+	int k;
+#endif
 
 	struct qm_sg_entry *sgt;
 	void *sgt_buf;
@@ -745,7 +767,19 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	const int nr_frags = skb_shinfo(skb)->nr_frags;
 
 	fd->format = qm_fd_sg;
+#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+	/* get a page frag to store the SGTable */
+	sgt_buf = netdev_alloc_frag(priv->tx_headroom +
+		sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES);
+	if (unlikely(!sgt_buf)) {
+		dev_err(dpa_bp->dev, "netdev_alloc_frag() failed\n");
+		return -ENOMEM;
+	}
 
+	/* it seems that the memory allocator does not zero the allocated mem */
+	memset(sgt_buf, 0, priv->tx_headroom +
+		sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES);
+#else
 	/* get a page frag to store the SGTable */
 	sgt_buf = netdev_alloc_frag(priv->tx_headroom +
 		sizeof(struct qm_sg_entry) * (1 + nr_frags));
@@ -756,6 +790,7 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 	memset(sgt_buf, 0, priv->tx_headroom +
 		sizeof(struct qm_sg_entry) * (1 + nr_frags));
+#endif
 
 	/* Enable L3/L4 hardware checksum computation.
 	 *
@@ -788,6 +823,68 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 	qm_sg_entry_set64(&sgt[0], addr);
 
+#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+	j = 0;
+	if (unlikely(HAS_DMA_ISSUE(skb->data, sg_len))) {
+		boundary = (void *)BOUNDARY_4K(skb->data, sg_len);
+		qm_sg_entry_set_len(&sgt[j], (u64)boundary - (u64)addr);
+
+		j++;
+		qm_sg_entry_set_bpid(&sgt[j], 0xff);
+		qm_sg_entry_set_offset(&sgt[j], 0);
+		qm_sg_entry_set_len(&sgt[j],
+			((u64)skb->data + (u64)sg_len) - (u64)boundary);
+		qm_sg_entry_set_ext(&sgt[j], 0);
+		qm_sg_entry_set_final(&sgt[j], 0);
+
+		/* keep the offset in the address */
+		qm_sg_entry_set64(&sgt[j], addr +
+				((u64)boundary - (u64)skb->data));
+	}
+	j++;
+
+	/* populate the rest of SGT entries */
+	for (i = 1; i <= nr_frags; i++, j++) {
+		frag = &skb_shinfo(skb)->frags[i - 1];
+		qm_sg_entry_set_bpid(&sgt[j], 0xff);
+		qm_sg_entry_set_offset(&sgt[j], 0);
+		qm_sg_entry_set_len(&sgt[j], frag->size);
+		qm_sg_entry_set_ext(&sgt[j], 0);
+
+		DPA_BUG_ON(!skb_frag_page(frag));
+		addr = skb_frag_dma_map(dpa_bp->dev, frag, 0, frag->size,
+					dma_dir);
+		if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
+			dev_err(dpa_bp->dev, "DMA mapping failed");
+			err = -EINVAL;
+			goto sg_map_failed;
+		}
+
+		/* keep the offset in the address */
+		qm_sg_entry_set64(&sgt[j], addr);
+
+		if (unlikely(HAS_DMA_ISSUE(frag, frag->size))) {
+			boundary = (void *)BOUNDARY_4K(frag, frag->size);
+			qm_sg_entry_set_len(&sgt[j], (u64)boundary - (u64)frag);
+			
+			j++;	
+			qm_sg_entry_set_bpid(&sgt[j], 0xff);
+			qm_sg_entry_set_offset(&sgt[j], 0);
+			qm_sg_entry_set_len(&sgt[j], ((u64)frag->size -
+						((u64)boundary - (u64)frag)));
+			qm_sg_entry_set_ext(&sgt[j], 0);
+
+			/* keep the offset in the address */
+			qm_sg_entry_set64(&sgt[j], addr +
+					((u64)boundary - (u64)frag));
+		}
+
+		if (i == nr_frags)
+			qm_sg_entry_set_final(&sgt[j], 1);
+		else
+			qm_sg_entry_set_final(&sgt[j], 0);
+#else
+
 	/* populate the rest of SGT entries */
 	for (i = 1; i <= nr_frags; i++) {
 		frag = &skb_shinfo(skb)->frags[i - 1];
@@ -812,6 +909,7 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 		/* keep the offset in the address */
 		qm_sg_entry_set64(&sgt[i], addr);
+#endif
 	}
 
 	fd->length20 = skb->len;
@@ -820,9 +918,15 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 	/* DMA map the SGT page */
 	buffer_start = (void *)sgt - priv->tx_headroom;
 	DPA_WRITE_SKB_PTR(skb, skbh, buffer_start, 0);
+#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+	addr = dma_map_single(dpa_bp->dev, buffer_start, priv->tx_headroom +
+			      sizeof(struct qm_sg_entry) * DPA_SGT_MAX_ENTRIES,
+			      dma_dir);
+#else
 	addr = dma_map_single(dpa_bp->dev, buffer_start, priv->tx_headroom +
 			      sizeof(struct qm_sg_entry) * (1 + nr_frags),
 			      dma_dir);
+#endif
 	if (unlikely(dma_mapping_error(dpa_bp->dev, addr))) {
 		dev_err(dpa_bp->dev, "DMA mapping failed");
 		err = -EINVAL;
@@ -837,11 +941,19 @@ int __hot skb_to_sg_fd(struct dpa_priv_s *priv,
 
 sgt_map_failed:
 sg_map_failed:
+#ifdef DPAA_LS1043A_DMA_4K_ISSUE
+	for (k = 0; k < j; k++) {
+		sg_addr = qm_sg_addr(&sgt[k]);
+		dma_unmap_page(dpa_bp->dev, sg_addr,
+			       qm_sg_entry_get_len(&sgt[k]), dma_dir);
+	}
+#else
 	for (j = 0; j < i; j++) {
 		sg_addr = qm_sg_addr(&sgt[j]);
 		dma_unmap_page(dpa_bp->dev, sg_addr,
 			       qm_sg_entry_get_len(&sgt[j]), dma_dir);
 	}
+#endif
 sg0_map_failed:
 csum_failed:
 	put_page(virt_to_head_page(sgt_buf));
-- 
2.1.0.27.g96db324

